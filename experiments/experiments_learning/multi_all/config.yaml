environment:
  A: [1., 1.]
  C: [1., 1.]
  mu_0 : 0.25
  sigma: 0.
  a_0: -1.
  N: 2
price_bounds: [1.0, 2.8]
optimization_cases: 
  - name: best_response_penalised
    agent:
      type: ExtortionAgent
      params:
        map_type: custom
        map_params_path: "../experiment_theory_strategies/outputs/best_response/best_response_params.txt"
    delay: 0.0
    n_competitions: 1
    T: 50
    skip_steps: 40
    penalty_coeff: 1
    penalty_slack: 0.00
    learning_agent_initial_price: 1.3
    extortion_agent_initial_price: 1.3
  - name: self_play
    agent:
      type: ExtortionAgentSelfPlay
      params:
        dummy: true
    delay: 0.5
    n_competitions: 1
    T: 100
    skip_steps: 80
    learning_agent_initial_price: 1.3
    extortion_agent_initial_price: 1.3
  - name: sw_uts
    agent:
      type: SW_UTS
      params:
        n_actions: 51
        n_neighbors: 2
        sliding_window: 50
        std: 0.01
    delay: 0.0
    n_competitions: 3
    T: 500
    skip_steps: 400
    learning_agent_initial_price: 1.3
    extortion_agent_initial_price: 1.3
    penalty_coeff: 1
    penalty_slack: 0.00
extortion_agent:
  type: ExtortionAgent
  theoretical_mapping_path: "../experiment_theory_strategies/outputs/tit_for_tat/tit_for_tat_up_prices.txt"
  theoretical_mapping_name: "Tit-For-Tat"
  nash_price: 1.473
  pareto_price: 1.925
  params:
    map_type: monotone_equispaced
    n_params: 14
optimization:
  maxfevals: 1000
  seed: 42
  sigma0: 0.8
  param_bounds: [0., 6.]


