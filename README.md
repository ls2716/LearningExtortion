# Learning to Extort Rational Opponents in Pricing Duopolies


The code supports the research presented in Chapter 4 of the PhD thesis:  
“Algorithmic Pricing in Competitive Markets”  
by Lukasz Sliwinski, 2025, University of Edinburgh.

---

## Installation

Create and activate a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate        # On Windows: .venv\Scripts\activate
```

Install the required packages:

```bash
pip install -r requirements.txt
```

Install in editable mode:

```bash
pip install -e .
```

## directory structure

The `src` directory contains the main code that defines the environment, agents, and experiment logic.

The actual experiments are located in the experiments folder and the plotting scripts in the plotting folder.

Because the source is installed in editable mode, you should run the experiments and plotting scripts from each respective folder containing the scripts, e.g.:

```bash
cd experiments/experiment_theory_constraint_line
python analyse_constraint_line.py
```

### Plotting scripts

- `plot_sample_strategy/plot_sample_strategy.py`: Generates a sample strategy using monotone equispaced parametrization.
- `plot_eps_greedy_strategy/plot_eps_greedy_strategy.py`: Plots the learned and theoretical extortion strategies against the epsilon-greedy opponent. The theoretical strategy is taken from `experiment_theory_strategies/extortion_mapping.txt` generated by `experiment_theory_strategies/compute_extortion_strategy.py`. The `optimised_params.txt` file contains the parameters of the learned strategy and is takend from `experiments_learning/eps_greedy/optimised_params.txt`.

### Experiments

The experiments are located in the experiments folder.

#### experiment_theory_constraint_line

`analyse_constraint_line.py` computes and plots the theoretical constraint line for extortion given the environment parameters.

#### experiment_theory_strategies

Contains the code for theoretical analysis of extortion strategies in the duopoly pricing environment.

- `compute_extortion_strategy.py`: Computes the theoretical extortion strategies against myopic agents: epsgreedy and unimodal epsgreedy bandits (latter requires increasing induced reward function). This code creates:
    - `extortion_mapping.txt` file used in plotting scripts,
    - `extortion_mapping.png` which visualises the mapping between opponent's prices and extortionate prices,
    - `unimodal_extortion_mapping.txt` file containing the extortion strategy against unimodal eps-greedy opponent.
    - `unimodal_extortion_mapping.png` visualising the unimodal extortion strategy.
- `compute_nash.py`: Computes the Nash equilibrium prices for the duopoly pricing environment.
- `compute_best_response.py`: Computes the best response mapping and results in:
    - `best_response_params.txt` which contains the strategy mapping opponent's prices to best response prices. This file is in the format that is used by input to the agent object.
    - `best_response_function.png` visualising the best response mapping.
- `compute_tit_for_tat.py`: Code which generates tit-for-tat strategy files:
    - `tit_for_tat_prices.png` plot of the price response of the tit-for-tat strategy
    - `tit_for_tat_rewards.png` plot of the rewards given that opponent also uses tit-for-tat
    - `tit_for_tat_params.txt` which contains the parameters of the tit-for-tat strategy (used by the agent class).
    - `tit_for_tat_prices.txt` which contains the price response of the tit-for-tat strategy.


#### experiment_stationary

This directory contains experiment where the leader optimises a stationary, mixed strategy i.e. returns a price from a distribution which does not change in time.

To reproduce the results first run the `train.py`, then `evaluate.py` and lastly the `plot_stationary.py` which will generate the results and the images.

#### experiments_learning

Main directory which contains all the learning experiments against a variety of learning opponents.

There are two main scripts used for the experiment: `train.py` which runs the training and `evaluate.py` which runs the evaluation.

##### Training

The training script `train.py` requires training instance specified by the training setup folder with a config of the training objective. Thus, for eps_greedy, it looks as follows:
```yaml
environment:
  A: [1., 1.]
  C: [1., 1.]
  mu_0 : 0.25
  sigma: 0.
  a_0: -1.
  N: 2
price_bounds: [1.0, 2.8]
optimization_cases: 
  - name: eps_greedy
    agent:
      type: EpsGreedyBandit
      params:
        epsilon: 0.5
        n_actions: 21
        decay: 0.995
    delay: 0.0
    n_competitions: 10
    T: 1000
    skip_steps: 800
    learning_agent_initial_price: 1.5
    extortion_agent_initial_price: 1.5
extortion_agent:
  type: ExtortionAgent
  theoretical_mapping_path: ../experiment_theory_strategies/extortion_mapping.txt
  params:
    map_type: monotone_equispaced
    n_params: 14
optimization:
  maxfevals: 1000
  seed: 42
  sigma0: 0.8
  param_bounds: [0., 6.]
```

Then:
- environment: specifies the duopoly pricing environment parameters.
- price_bounds: specifies the minimum and maximum prices that agents can choose.
- optimization_cases: list of training cases with different opponents. Each case specifies:
    - name: name of the opponent
    - agent: configuration of the opponent agent
    - delay: observation delay for the learning agent
    - n_competitions: number of competitions to average results over
    - T: number of time steps in each competition
    - skip_steps: number of initial steps to skip when computing the average reward
    - learning_agent_initial_price: initial price for the learning agent
    - extortion_agent_initial_price: initial price for the extortion agent
- extortion_agent: configuration of the extortion agent including the mapping setup and theoretical path for comparison in the evaluation (not used in training).
- optimization: parameters for the CMA-ES optimization including maximum function evaluations, random seed, initial standard deviation, and parameter bounds.

To run the training, set up the folder with the config, change the path in the `train.py` script:
```python
if __name__ == "__main__":
    
    directory = "eps_greedy" # specify the training setup folder
    config_path = directory + "/config.yaml"
```

Then run:
```bash
python train.py
```

This will create the results folder with:
- `optimised_params.txt`: file containing the learned strategy parameters.
- `results.json`: file containing the detailed results of the training process.

##### Evaluation

The evaluation script `evaluate.py` uses the learned parameters to evaluate the performance of the extortion agent against the specified opponent. The setup is similar to the training setup, but it uses the learned parameters from the training phase and evaluation config in the `experiments_learning` folder.
The config file follows exactly the same structure as the training config, but the optimisation cases specify evaluation cases and the evaluation config is common for all cases and located in the `experiments_learning/evaluation_config.yaml` file.

To run the evaluation, set up the folder with the config, change the path in the `evaluate.py` script:
```python
if __name__ == "__main__":

    directory = "multi_all" # specify the evaluation setup folder
    evaluation_config_path = "evaluation_config.yaml"
    evaluation_config = load_config(evaluation_config_path)
```

Then run:
```bash
python evaluate.py
```
This will create:
- `evaluation_results.json`: file containing the detailed results of the evaluation process.
- `images` folder with plots visualising the results:
    - `response_and_induced_rewards.png`: plot showing the learned strategy response and induced rewards.
    - individual folders with plots for each optimisation case showing the evolution of the competition (prices and rewards over time).

##### Opponent-Aware Experiments (oa)

There are two additional files: `evaluate_oa.py` and `evaluation_oa_config.yaml` which runs the evaluation of the extortion agent against an opponent-aware learning agent. Running the script results in another image folder with results specific to the opponent-aware learning agent `modelbased_opponentaware`.

#### experiment_delta_sweep_epsgreedy

This directory contains the experiments where the extortion agent learns against an eps_greedy opponent with varying delta parameter which controls the response speed of the extortion agent (delta=0 corresponds to no delay, delta=1 corresponds to 1 time step delay). The wokrkflow is identical to the learning experiments in the `experiments_learning` folder with additional plotting and analysis scripts: `compare_strategies.py` and `get_table.py` which generate comparison plots of the learned strategies and create a LateX table summarising the results.

#### experiment_delta_sweep_sw_uts

Same as above but against the Sliding-Window Unimodal Thompson Sampling (SW-UTS) bandit opponent.


